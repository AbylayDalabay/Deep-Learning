{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5w0SpdwIqCGW6hVDFZyTx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbylayDalabay/Deep-Learning/blob/main/Medical_Image_Classification_project_proposal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Medical Image Classification**"
      ],
      "metadata": {
        "id": "H5FhqdQ6ZtNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Title:* Chest X-Ray Images (Pneumonia) Classification\n",
        "\n",
        "*Team members*: Dalabay Abylay\n",
        "\n",
        "*Link to the dataset:* https://data.mendeley.com/datasets/rscbjbr9sj/2"
      ],
      "metadata": {
        "id": "rJ-Bz-faZwmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Architecture or algorithms that you are going to use:*\n",
        "\n",
        "1) Pretrained model ResNet\n",
        "  Between different depths of ResNet we can use resnet101\n",
        "  (Other case is VGG)\n",
        "They can be downloaded from https://pytorch.org/vision/0.8/_modules/torchvision/models/vgg.html"
      ],
      "metadata": {
        "id": "ZRRP8X8rgaAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import skimage\n",
        "from skimage import io, transform\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "model_pre = models.vgg16()\n",
        "model_pre.load_state_dict(torch.load(\"../input/pytorch-pretrained-models/vgg16-397923af.pth\"))\n",
        "# it should be saved"
      ],
      "metadata": {
        "id": "ogi1IkKplIOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sizes, classes, class_names = [ ... will be declared]"
      ],
      "metadata": {
        "id": "9xDdxV2Flf_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_pre.features.parameters():\n",
        "    param.required_grad = False\n",
        "\n",
        "num_features = model_pre.classifier[6].in_features\n",
        "features = list(model_pre.classifier.children())[:-1]\n",
        "features.extend([nn.Linear(num_features, len(class_names))])\n",
        "model_pre.classifier = nn.Sequential(*features)\n",
        "\n",
        "print(model_pre)"
      ],
      "metadata": {
        "id": "KDSfCXnchJ8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG(\n",
        "  (features): Sequential(\n",
        "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (1): ReLU(inplace=True)\n",
        "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (3): ReLU(inplace=True)\n",
        "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (6): ReLU(inplace=True)\n",
        "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (8): ReLU(inplace=True)\n",
        "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (11): ReLU(inplace=True)\n",
        "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (13): ReLU(inplace=True)\n",
        "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (15): ReLU(inplace=True)\n",
        "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (18): ReLU(inplace=True)\n",
        "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (20): ReLU(inplace=True)\n",
        "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (22): ReLU(inplace=True)\n",
        "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (25): ReLU(inplace=True)\n",
        "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (27): ReLU(inplace=True)\n",
        "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "    (29): ReLU(inplace=True)\n",
        "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
        "  )\n",
        "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
        "  (classifier): Sequential(\n",
        "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
        "    (1): ReLU(inplace=True)\n",
        "    (2): Dropout(p=0.5, inplace=False)\n",
        "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
        "    (4): ReLU(inplace=True)\n",
        "    (5): Dropout(p=0.5, inplace=False)\n",
        "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
        "  )\n",
        ")"
      ],
      "metadata": {
        "id": "QciQ1ZfxqZ26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be using ResNet, VGG versions, DenseNet"
      ],
      "metadata": {
        "id": "x7Ob5Dm9q4Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fwopgh5q5Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing\n",
        "  1. If labels inbalanced, try to equal them by croping max or increasing minimum\n",
        "  2. Image Augmentation: rotation, flipping, cropping, scaling, translation"
      ],
      "metadata": {
        "id": "IFGqXxWHrNz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) CNN architecture by hand"
      ],
      "metadata": {
        "id": "9hSDNc76ZnRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PrimaryModel(nn.Module):\n",
        "    def __init__(self, input_shape=(1, 224, 224)):\n",
        "        super(PrimaryModel, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            self.num_flat_features = np.prod(self.features(torch.zeros(1, *input_shape)).shape)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.num_flat_features, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, self.num_flat_features)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "JZc5zBlqzeEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation methods:\n",
        "+ Training and validation Loss graph\n",
        "+ Training and Validation accuracy\n",
        "+ confusion matrix"
      ],
      "metadata": {
        "id": "HJZL2S0ta2Ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will be comparing VGG to my own CNN results\n",
        "\n",
        "More likely there will be changed some pooling layers"
      ],
      "metadata": {
        "id": "Hbj-2JYibWi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "+ https://neurohive.io/ru/vidy-nejrosetej/vgg16-model/\n",
        "+ https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data\n",
        "+ https://www.kaggle.com/code/pankul/image-classification-w-vgg16-weights"
      ],
      "metadata": {
        "id": "TjJcPwTqb1jD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKXfF7WCb2o4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}